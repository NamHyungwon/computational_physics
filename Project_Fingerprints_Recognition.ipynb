{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[242 243 245 246 246 253 255 254 253 252 255 255 252 252 254 254 253 254]\n",
      " [241 241 244 245 252 237 211 202 191 189 183 218 249 253 253 253 254 252]\n",
      " [236 236 240 242 216 183 167 178 166 177 172 162 197 250 252 252 253 252]\n",
      " [234 235 238 202 176 171 170 181 179 174 186 194 175 211 255 250 252 252]\n",
      " [233 236 224 175 178 172 173 167 178 174 176 191 193 195 250 251 251 251]\n",
      " [231 237 213 178 188 179 176 179 178 176 186 198 199 205 246 252 251 251]\n",
      " [232 236 212 184 184 180 174 178 177 182 183 190 195 205 243 251 250 251]\n",
      " [236 229 199 180 172 178 162 167 166 180 179 190 189 215 249 249 250 251]\n",
      " [238 216 187 180 179 173 157 156 163 178 176 190 193 207 245 249 248 249]\n",
      " [235 210 186 187 184 171 166 156 170 176 179 185 192 200 245 248 248 249]\n",
      " [234 217 195 188 183 173 162 161 169 176 178 178 182 196 244 246 247 248]\n",
      " [238 222 203 187 191 180 161 158 164 165 165 175 178 207 245 244 246 247]\n",
      " [240 226 205 197 190 175 158 160 161 161 158 173 171 212 246 244 245 246]\n",
      " [240 232 218 211 198 185 185 172 170 166 160 173 183 226 244 244 244 245]\n",
      " [239 234 218 215 209 191 176 171 162 162 158 177 198 242 242 242 244 245]\n",
      " [237 242 222 219 211 193 199 192 174 182 170 185 230 246 243 244 244 245]\n",
      " [232 238 240 230 226 213 216 219 208 196 189 219 249 244 244 244 243 243]\n",
      " [225 225 240 240 241 244 243 236 234 238 239 249 245 245 244 243 243 242]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def average_hash(fname, size=18):\n",
    "    img=Image.open(fname)\n",
    "    img=img.convert('L')\n",
    "    img=img.resize((size,size), Image.ANTIALIAS)\n",
    "    pixel_data = img.getdata()\n",
    "    pixels = np.array(pixel_data)\n",
    "    pixels = pixels.reshape((size,size))\n",
    "    #avg=pixels.mean()\n",
    "    #diff=1*(pixels>avg)\n",
    "    return pixels\n",
    "\n",
    "\n",
    "ahash_2=average_hash('확실한지문.jpg')\n",
    "ahash_3=average_hash('덜확실한지문.jpg')\n",
    "\n",
    "#a = ahash_3.reshape(1,-1)\n",
    "#b = ahash_4.reshape(1,-1)\n",
    "\n",
    "\n",
    "\n",
    "''''print(ahash_1)\n",
    "print(ahash_2)\n",
    "print(ahash_3)'''\n",
    "\n",
    "print(ahash_3)\n",
    "#print(ahash_2)\n",
    "#print((a !=b).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n",
      "[[[160 162 161]\n",
      "  [162 164 163]\n",
      "  [163 165 164]\n",
      "  ...\n",
      "  [176 178 177]\n",
      "  [174 176 175]\n",
      "  [173 175 174]]\n",
      "\n",
      " [[165 167 166]\n",
      "  [164 166 165]\n",
      "  [163 165 164]\n",
      "  ...\n",
      "  [176 178 177]\n",
      "  [174 176 175]\n",
      "  [175 177 176]]\n",
      "\n",
      " [[164 166 165]\n",
      "  [168 170 169]\n",
      "  [167 169 168]\n",
      "  ...\n",
      "  [175 177 176]\n",
      "  [177 179 178]\n",
      "  [175 177 176]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[170 181 183]\n",
      "  [166 177 179]\n",
      "  [169 180 182]\n",
      "  ...\n",
      "  [186 192 192]\n",
      "  [190 196 196]\n",
      "  [189 195 195]]\n",
      "\n",
      " [[173 184 186]\n",
      "  [169 180 182]\n",
      "  [170 181 183]\n",
      "  ...\n",
      "  [188 194 194]\n",
      "  [188 194 194]\n",
      "  [189 195 195]]\n",
      "\n",
      " [[168 179 181]\n",
      "  [169 180 182]\n",
      "  [167 178 180]\n",
      "  ...\n",
      "  [189 195 195]\n",
      "  [189 195 195]\n",
      "  [189 195 195]]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "with open(\"3.jpg\",\"rb\") as file:\n",
    "    img = Image.open(file)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((28,28))\n",
    "    data = np.asarray(img)\n",
    "    print(len(list(data)))\n",
    "    print(len(list(data[0])))\n",
    "    print(data)\n",
    "    img.save(\"test3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ok', 4875, 4875, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#분류 대상 카테고리 선택하기\n",
    "caltech_dir=\"./지문\"\n",
    "categories = [\"형원검지\",\"재영검지\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#이미지 크기 지정\n",
    "image_w=64\n",
    "image_h=64\n",
    "pixels = image_w * image_h *3\n",
    "\n",
    "#이미지 데이터 읽어 들이기\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    #레이블 지정\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx]=1\n",
    "    #이미지\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    for i, f in enumerate(files):\n",
    "        length = len(files)\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "        if i < length * 0.7:\n",
    "            X_train.append(data)\n",
    "            y_train.append(label)\n",
    "        else:\n",
    "            X_test.append(data)\n",
    "            y_test.append(label)\n",
    "        for angle in range(-30,30,5):\n",
    "            img2 = img.rotate(angle)\n",
    "            data = np.asarray(img2)\n",
    "            if i < length * 0.7:\n",
    "                X_train.append(data)\n",
    "                y_train.append(label)\n",
    "            #img2=img2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            #data = np.asarray(img2)\n",
    "            #if i < length * 0.7:\n",
    "                #X_train.append(data)\n",
    "                #y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./지문/f.npy\", xy)\n",
    "print(\"ok\", len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (4875, 64, 64, 3))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", input_shape=(64, 64, 3...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:48: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4875/4875 [==============================] - 3s 540us/step - loss: 0.5800 - acc: 0.7659\n",
      "Epoch 2/30\n",
      "4875/4875 [==============================] - 2s 422us/step - loss: 0.3334 - acc: 0.8646\n",
      "Epoch 3/30\n",
      "4875/4875 [==============================] - 2s 381us/step - loss: 0.2059 - acc: 0.9383\n",
      "Epoch 4/30\n",
      "4875/4875 [==============================] - 2s 392us/step - loss: 0.1174 - acc: 0.9633\n",
      "Epoch 5/30\n",
      "4875/4875 [==============================] - 2s 407us/step - loss: 0.0823 - acc: 0.9733\n",
      "Epoch 6/30\n",
      "4875/4875 [==============================] - 2s 373us/step - loss: 0.0665 - acc: 0.9822\n",
      "Epoch 7/30\n",
      "4875/4875 [==============================] - 2s 388us/step - loss: 0.0763 - acc: 0.9785\n",
      "Epoch 8/30\n",
      "4875/4875 [==============================] - 2s 386us/step - loss: 0.0457 - acc: 0.9883\n",
      "Epoch 9/30\n",
      "4875/4875 [==============================] - 2s 391us/step - loss: 0.0462 - acc: 0.9861\n",
      "Epoch 10/30\n",
      "4875/4875 [==============================] - 2s 389us/step - loss: 0.0435 - acc: 0.9924\n",
      "Epoch 11/30\n",
      "4875/4875 [==============================] - 2s 389us/step - loss: 0.0590 - acc: 0.9863\n",
      "Epoch 12/30\n",
      "4875/4875 [==============================] - 2s 395us/step - loss: 0.0420 - acc: 0.9902\n",
      "Epoch 13/30\n",
      "4875/4875 [==============================] - 2s 382us/step - loss: 0.0342 - acc: 0.9922\n",
      "Epoch 14/30\n",
      "4875/4875 [==============================] - 2s 384us/step - loss: 0.0366 - acc: 0.9926\n",
      "Epoch 15/30\n",
      "4875/4875 [==============================] - 2s 385us/step - loss: 0.0304 - acc: 0.9924\n",
      "Epoch 16/30\n",
      "4875/4875 [==============================] - 2s 396us/step - loss: 0.0429 - acc: 0.9938\n",
      "Epoch 17/30\n",
      "4875/4875 [==============================] - 2s 432us/step - loss: 0.0422 - acc: 0.9914\n",
      "Epoch 18/30\n",
      "4875/4875 [==============================] - 2s 427us/step - loss: 0.0318 - acc: 0.9941\n",
      "Epoch 19/30\n",
      "4875/4875 [==============================] - 2s 430us/step - loss: 0.0295 - acc: 0.9934\n",
      "Epoch 20/30\n",
      "4875/4875 [==============================] - 2s 429us/step - loss: 0.0215 - acc: 0.9965\n",
      "Epoch 21/30\n",
      "4875/4875 [==============================] - 2s 454us/step - loss: 0.0523 - acc: 0.9914\n",
      "Epoch 22/30\n",
      "4875/4875 [==============================] - 2s 402us/step - loss: 0.0313 - acc: 0.9941\n",
      "Epoch 23/30\n",
      "4875/4875 [==============================] - 2s 434us/step - loss: 0.0308 - acc: 0.9953\n",
      "Epoch 24/30\n",
      "4875/4875 [==============================] - 2s 392us/step - loss: 0.0207 - acc: 0.9959\n",
      "Epoch 25/30\n",
      "4875/4875 [==============================] - 2s 383us/step - loss: 0.0393 - acc: 0.9922\n",
      "Epoch 26/30\n",
      "4875/4875 [==============================] - 2s 393us/step - loss: 0.0425 - acc: 0.9938\n",
      "Epoch 27/30\n",
      "4875/4875 [==============================] - 2s 417us/step - loss: 0.0317 - acc: 0.9955\n",
      "Epoch 28/30\n",
      "4875/4875 [==============================] - 2s 381us/step - loss: 0.0395 - acc: 0.9938\n",
      "Epoch 29/30\n",
      "4875/4875 [==============================] - 2s 407us/step - loss: 0.0333 - acc: 0.9959\n",
      "Epoch 30/30\n",
      "4875/4875 [==============================] - 2s 378us/step - loss: 0.0437 - acc: 0.9938\n",
      "160/160 [==============================] - 0s 1ms/step\n",
      "('loss=', 1.0960467022869125e-07)\n",
      "('accuracy=', 1.0)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "#카테고리 지정하기\n",
    "categories = [\"형원검지\",\"재영검지\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#이미지 크기 지정하기\n",
    "image_w =64\n",
    "image_h =64\n",
    "\n",
    "#데이터 열기\n",
    "X_train, X_test, y_train, y_test = np.load(\"./지문/f.npy\")\n",
    "\n",
    "#데이터 정규화하기\n",
    "X_train = X_train.astype(\"float\") / 256\n",
    "X_test = X_test.astype(\"float\") / 256\n",
    "print('X_train shape:', X_train.shape)\n",
    "\n",
    "#모델 구축하기\n",
    "model1 = Sequential()\n",
    "model1.add(Convolution2D(32, 3, 3,border_mode='same', input_shape=X_train.shape[1:]))\n",
    "model1.add(Activation('relu')) \n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Convolution2D(64, 3, 3))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "model1.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model1.add(Activation('relu')) #relu\n",
    "model1.add(Convolution2D(32, 3, 3))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(512))\n",
    "model1.add(Activation('relu')) #relu\n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax')) #softmax\n",
    "model1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "#모델 훈련하기\n",
    "model1.fit(X_train, y_train, batch_size=25,nb_epoch=30)\n",
    "\n",
    "#모델 평가하기\n",
    "score1 = model1.evaluate(X_test, y_test)\n",
    "print('loss=', score1[0])\n",
    "print('accuracy=', score1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측하기\n",
    "pre = model1.predict(X_test)\n",
    "#예측결과테스트하기\n",
    "for i, v in enumerate(pre):\n",
    "    pre_ans = v.argmax() #예측한 레이블\n",
    "    ans = y_test[i].argmax() #정답 레이블\n",
    "    dat = X_test[i] #이미지 데이터\n",
    "    if ans == pre_ans: continue\n",
    "    #예측이 틀리면 무엇이 틀렸는지 출력하기\n",
    "    print(\"[NG]\", categories[pre_ans], \"!=\", categories[ans])\n",
    "    print(v)\n",
    "    #이미지 출력하기\n",
    "    fname = \"./지문/error1/\" + str(i) + \"-\" + categories[pre_ans] + \"-ne-\"+categories[ans] + \".JPG\"\n",
    "    dat*=256\n",
    "    img = Image.fromarray(np.uint8(dat))\n",
    "    img.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 13, 13, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 703,074\n",
      "Trainable params: 703,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ok', 4617, 4617, 218, 218)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#분류 대상 카테고리 선택하기\n",
    "caltech_dir=\"./지문\"\n",
    "categories2 = [\"형원검지\",\"재영검지\",\"형원중지\",\"재영중지\"]\n",
    "nb_classes2 = len(categories2)\n",
    "\n",
    "#이미지 크기 지정\n",
    "image_w=64\n",
    "image_h=64\n",
    "pixels = image_w * image_h *3\n",
    "\n",
    "#이미지 데이터 읽어 들이기\n",
    "X2_train = []\n",
    "y2_train = []\n",
    "X2_test = []\n",
    "y2_test = []\n",
    "\n",
    "\n",
    "for idx, cat in enumerate(categories2):\n",
    "    #레이블 지정\n",
    "    label = [0 for i in range(nb_classes2)]\n",
    "    label[idx]=1\n",
    "    #이미지\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    for i, f in enumerate(files):\n",
    "        length = len(files)\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "        if i < length * 0.7:\n",
    "            X2_train.append(data)\n",
    "            y2_train.append(label)\n",
    "        else:\n",
    "            X2_test.append(data)\n",
    "            y2_test.append(label)\n",
    "        for angle in range(-20,20,5): #-25 25 5 에서 바꿈\n",
    "            img2 = img.rotate(angle)\n",
    "            data = np.asarray(img2)\n",
    "            if i < length * 0.7:\n",
    "                X2_train.append(data)\n",
    "                y2_train.append(label)\n",
    "            #img2=img2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            #data = np.asarray(img2)\n",
    "            #if i < length * 0.7:\n",
    "                #X2_train.append(data)\n",
    "                #y2_train.append(label)\n",
    "X2_train = np.array(X2_train)\n",
    "y2_train = np.array(y2_train)\n",
    "X2_test = np.array(X2_test)\n",
    "y2_test = np.array(y2_test)\n",
    "\n",
    "xy2 = (X2_train, X2_test, y2_train, y2_test)\n",
    "np.save(\"./지문/f2.npy\", xy2)\n",
    "print(\"ok\", len(X2_train), len(y2_train), len(X2_test), len(y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X2_train shape:', (4617, 64, 64, 3))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", input_shape=(64, 64, 3...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:45: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4617/4617 [==============================] - 3s 567us/step - loss: 0.4762 - acc: 0.7923\n",
      "Epoch 2/30\n",
      "4617/4617 [==============================] - 2s 374us/step - loss: 0.1973 - acc: 0.9205\n",
      "Epoch 3/30\n",
      "4617/4617 [==============================] - 2s 388us/step - loss: 0.1061 - acc: 0.9650\n",
      "Epoch 4/30\n",
      "4617/4617 [==============================] - 2s 420us/step - loss: 0.0696 - acc: 0.9758\n",
      "Epoch 5/30\n",
      "4617/4617 [==============================] - 2s 418us/step - loss: 0.0448 - acc: 0.9856\n",
      "Epoch 6/30\n",
      "4617/4617 [==============================] - 2s 388us/step - loss: 0.0401 - acc: 0.9870\n",
      "Epoch 7/30\n",
      "4617/4617 [==============================] - 2s 386us/step - loss: 0.0390 - acc: 0.9886\n",
      "Epoch 8/30\n",
      "4617/4617 [==============================] - 2s 435us/step - loss: 0.0294 - acc: 0.9901\n",
      "Epoch 9/30\n",
      "4617/4617 [==============================] - 2s 408us/step - loss: 0.0329 - acc: 0.9919\n",
      "Epoch 10/30\n",
      "4617/4617 [==============================] - 2s 381us/step - loss: 0.0260 - acc: 0.9930\n",
      "Epoch 11/30\n",
      "4617/4617 [==============================] - 2s 384us/step - loss: 0.0377 - acc: 0.9893\n",
      "Epoch 12/30\n",
      "4617/4617 [==============================] - 2s 435us/step - loss: 0.0209 - acc: 0.9955\n",
      "Epoch 13/30\n",
      "4617/4617 [==============================] - 2s 418us/step - loss: 0.0178 - acc: 0.9945\n",
      "Epoch 14/30\n",
      "4617/4617 [==============================] - 2s 410us/step - loss: 0.0210 - acc: 0.9945\n",
      "Epoch 15/30\n",
      "4617/4617 [==============================] - 2s 392us/step - loss: 0.0163 - acc: 0.9950\n",
      "Epoch 16/30\n",
      "4617/4617 [==============================] - 2s 393us/step - loss: 0.0195 - acc: 0.9950\n",
      "Epoch 17/30\n",
      "4617/4617 [==============================] - 2s 386us/step - loss: 0.0270 - acc: 0.9952\n",
      "Epoch 18/30\n",
      "4617/4617 [==============================] - 2s 410us/step - loss: 0.0156 - acc: 0.9958\n",
      "Epoch 19/30\n",
      "4617/4617 [==============================] - 2s 400us/step - loss: 0.0247 - acc: 0.9951\n",
      "Epoch 20/30\n",
      "4617/4617 [==============================] - 2s 411us/step - loss: 0.0142 - acc: 0.9962\n",
      "Epoch 21/30\n",
      "4617/4617 [==============================] - 2s 396us/step - loss: 0.0235 - acc: 0.9961\n",
      "Epoch 22/30\n",
      "4617/4617 [==============================] - 2s 383us/step - loss: 0.0148 - acc: 0.9975\n",
      "Epoch 23/30\n",
      "4617/4617 [==============================] - 2s 404us/step - loss: 0.0297 - acc: 0.9953\n",
      "Epoch 24/30\n",
      "4617/4617 [==============================] - 2s 417us/step - loss: 0.0149 - acc: 0.9972\n",
      "Epoch 25/30\n",
      "4617/4617 [==============================] - 2s 382us/step - loss: 0.0350 - acc: 0.9957\n",
      "Epoch 26/30\n",
      "4617/4617 [==============================] - 2s 372us/step - loss: 0.0227 - acc: 0.9960\n",
      "Epoch 27/30\n",
      "4617/4617 [==============================] - 2s 372us/step - loss: 0.0163 - acc: 0.9965\n",
      "Epoch 28/30\n",
      "4617/4617 [==============================] - 2s 382us/step - loss: 0.0158 - acc: 0.9963\n",
      "Epoch 29/30\n",
      "4617/4617 [==============================] - 2s 420us/step - loss: 0.0196 - acc: 0.9959\n",
      "Epoch 30/30\n",
      "4617/4617 [==============================] - 2s 382us/step - loss: 0.0122 - acc: 0.9975\n",
      "218/218 [==============================] - 0s 1ms/step\n",
      "('loss=', 0.2121906796328424)\n",
      "('accuracy=', 0.9747706422018348)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "categories2 = [\"형원검지\",\"재영검지\",\"형원중지\",\"재영중지\"]\n",
    "nb_classes2 = len(categories2)\n",
    "\n",
    "image_w =64\n",
    "image_h =64\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = np.load(\"./지문/f2.npy\")\n",
    "\n",
    "X2_train = X2_train.astype(\"float\") / 256\n",
    "X2_test = X2_test.astype(\"float\") / 256\n",
    "print('X2_train shape:', X2_train.shape)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Convolution2D(32, 3, 3,border_mode='same', input_shape=X2_train.shape[1:]))\n",
    "model2.add(Activation('relu')) # relu\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model2.add(Activation('relu')) #relu\n",
    "model2.add(Convolution2D(64, 3, 3))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model2.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model2.add(Activation('relu')) #relu\n",
    "model2.add(Convolution2D(32, 3, 3))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "# 하나 추가함\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(512))\n",
    "model2.add(Activation('relu')) #relu\n",
    "model2.add(Dense(nb_classes2))\n",
    "model2.add(Activation('softmax')) #softmax\n",
    "model2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "history=model2.fit(X2_train, y2_train, batch_size=25,nb_epoch=30) #epoch 50에서 바꿈\n",
    "\n",
    "score2 = model2.evaluate(X2_test, y2_test)\n",
    "print('loss=', score2[0])\n",
    "print('accuracy=', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history.history['acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "acc_values = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss, 'black', label='train_loss')\n",
    "plt.plot(epochs, acc, 'blue', label='train_acc')\n",
    "\n",
    "plt.title('Training loss and accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[4.3121397e-01 1.7630142e-05 5.6876838e-01 2.3721444e-25]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[1.3023482e-02 1.0374905e-18 9.8697656e-01 1.1585423e-28]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[2.4230772e-07 8.5342183e-13 9.9999976e-01 3.2864789e-27]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[1.11297183e-02 1.14897405e-17 9.88870263e-01 2.00533238e-35]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[1.2722713e-13 4.4780899e-27 1.0000000e+00 0.0000000e+00]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[7.2164930e-02 1.9427035e-25 9.2783511e-01 0.0000000e+00]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[1.00421525e-10 1.77281018e-17 1.00000000e+00 0.00000000e+00]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[6.2630227e-07 6.6798028e-24 9.9999940e-01 0.0000000e+00]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[3.4053999e-12 8.3382845e-22 1.0000000e+00 0.0000000e+00]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80')\n",
      "[3.9347711e-01 1.0016279e-06 6.0652184e-01 3.8975390e-10]\n",
      "('[NG]', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xea\\xb2\\x80\\xec\\xa7\\x80', '!=', '\\xed\\x98\\x95\\xec\\x9b\\x90\\xec\\xa4\\x91\\xec\\xa7\\x80')\n",
      "[6.8502790e-01 3.3454539e-10 3.1497207e-01 2.5964298e-20]\n"
     ]
    }
   ],
   "source": [
    "#예측하기\n",
    "pre2 = model2.predict(X2_test)\n",
    "#예측결과테스트하기\n",
    "for i, v in enumerate(pre2):\n",
    "    pre2_ans = v.argmax()\n",
    "    ans2 = y2_test[i].argmax()\n",
    "    dat2 = X2_test[i]\n",
    "    if ans2 == pre2_ans: continue\n",
    "    #예측이 틀리면 무엇이 틀렸는지 출력하기\n",
    "    print(\"[NG]\", categories2[pre2_ans], \"!=\", categories2[ans2])\n",
    "    print(v)\n",
    "    #이미지 출력하기\n",
    "    fname2 = \"./지문/error2/\" + str(i) + \"-\" + categories2[pre2_ans] + \"-ne-\"+categories2[ans2] + \".JPG\"\n",
    "    dat2*=256\n",
    "    img = Image.fromarray(np.uint8(dat2))\n",
    "    img.save(fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 30, 30, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 13, 13, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 704,100\n",
      "Trainable params: 704,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 ./지문/imgs_others_test/재영검지2.jpg이미지는 재영검지으로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/재영검지1.jpg이미지는 재영검지으로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/재영중지2.jpg이미지는 재영중지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/재영중지1.jpg이미지는 재영중지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/형원검지12.jpg이미지는 형원검지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/형원검지11.jpg이미지는 형원검지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/형원중지2.jpg이미지는 형원중지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/형원중지1.jpg이미지는 형원중지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원중지4.jpg이미지는 형원검지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원중지3.jpg이미지는 형원중지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원중지2.jpg이미지는 형원중지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원중지1.jpg이미지는 형원중지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원검지4.jpg이미지는 형원검지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원검지3.jpg이미지는 형원검지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원검지2.jpg이미지는 형원검지로 추정됩니다.\n",
      "해당 ./지문/imgs_others_test/sample_형원검지1.jpg이미지는 형원검지로 추정됩니다.\n"
     ]
    }
   ],
   "source": [
    "#새로운 이미지 파일들 넣어서 어떤 카테고리로 분류하는지 \n",
    "\n",
    "model2.save(\"tensorflow_model2.h5\") #model 저장하기\n",
    "\n",
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "caltech_dir = \"./지문/imgs_others_test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(caltech_dir+\"/*.*\")\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "from tensorflow import keras\n",
    "model = load_model('./tensorflow_model2.h5')\n",
    "\n",
    "prediction = model2.predict(X)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    #print(i)\n",
    "    #print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"형원검지\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"재영검지\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"형원중지\"\n",
    "    else: pre_ans_str = \"재영중지\"\n",
    "    \n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
